{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "301bf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import scipy.optimize\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n",
    "import string\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "31cab31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "33f967ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "3e25a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readGz(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "f88efc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCSV(path):\n",
    "    f = gzip.open(path, 'rt')\n",
    "    f.readline()\n",
    "    for l in f:\n",
    "        u,b,r = l.strip().split(',')\n",
    "        r = int(r)\n",
    "        yield u,b,r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "3a5f39ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b3b16eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some data structures that will be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "09ac1dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "allRatings = []\n",
    "for l in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    allRatings.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "e4717806",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200000"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allRatings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ca3c2a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingsTrain = allRatings[:190000]\n",
    "ratingsValid = allRatings[190000:]\n",
    "ratingsPerUser = defaultdict(list)\n",
    "ratingsPerItem = defaultdict(list)\n",
    "for u,b,r in ratingsTrain:\n",
    "    ratingsPerUser[u].append((b,r))\n",
    "    ratingsPerItem[b].append((u,r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "bcf70975",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Rating prediction (CSE258 only)                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "95b960a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "46c28bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(pred_rs, actual_rs):\n",
    "    differences = [(pred_r - actual_r)**2 for pred_r, actual_r in zip(pred_rs,actual_rs)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "6d69e40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(user, item):\n",
    "    return alpha + userBiases[user] + itemBiases[item]\n",
    "\n",
    "def unpack(theta):\n",
    "    global alpha\n",
    "    global userBiases\n",
    "    global itemBiases\n",
    "    alpha = theta[0]\n",
    "    userBiases = dict(zip(users, theta[1:nUsers+1]))\n",
    "    itemBiases = dict(zip(items, theta[1+nUsers:]))\n",
    "    \n",
    "def cost(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    predictions = [prediction(u, b) for u, b, r in ratingsTrain]\n",
    "    cost = MSE(predictions, labels)\n",
    "    for u in userBiases:\n",
    "        cost += lamb*userBiases[u]**2\n",
    "    for i in itemBiases:\n",
    "        cost += lamb*itemBiases[i]**2\n",
    "    return cost\n",
    "\n",
    "def derivative(theta, labels, lamb):\n",
    "    unpack(theta)\n",
    "    N = len(ratingsTrain)\n",
    "    dalpha = 0\n",
    "    dUserBiases = defaultdict(float)\n",
    "    dItemBiases = defaultdict(float)\n",
    "    for u, b, r in ratingsTrain:\n",
    "        u,i = u, b\n",
    "        pred = prediction(u, i)\n",
    "        diff = pred - r\n",
    "        dalpha += 2/N*diff\n",
    "        dUserBiases[u] += 2/N*diff\n",
    "        dItemBiases[i] += 2/N*diff\n",
    "    for u in userBiases:\n",
    "        dUserBiases[u] += 2*lamb*userBiases[u]\n",
    "    for i in itemBiases:\n",
    "        dItemBiases[i] += 2*lamb*itemBiases[i]\n",
    "    dtheta = [dalpha] + [dUserBiases[u] for u in users] + [dItemBiases[i] for i in items]\n",
    "    return np.array(dtheta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "754eb9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 3.68666543e+00, -2.49296541e-04,  9.89577346e-06, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]),\n",
       " 1.7414265936521982,\n",
       " {'grad': array([ 4.45195907e-07, -9.02043271e-08, -6.31721614e-09, ...,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]),\n",
       "  'task': 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL',\n",
       "  'funcalls': 4,\n",
       "  'nit': 2,\n",
       "  'warnflag': 0})"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train\n",
    "lamb = 1\n",
    "\n",
    "labels = [r[2] for r in ratingsTrain]\n",
    "ratingMean = sum(labels)/len(ratingsTrain)\n",
    "\n",
    "users = list(ratingsPerUser.keys())\n",
    "items = list(ratingsPerItem.keys())\n",
    "nUsers = len(users)\n",
    "nItems = len(items)\n",
    "\n",
    "alpha = ratingMean\n",
    "\n",
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "\n",
    "scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                             derivative, args = (labels, lamb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "a0d1e39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#validating\n",
    "preds = []\n",
    "valid_labels  = []\n",
    "\n",
    "for u, b, r in ratingsValid:\n",
    "    valid_labels.append(r)\n",
    "    if u in userBiases and b in itemBiases:\n",
    "        preds.append(prediction(u, b))\n",
    "    else:\n",
    "        preds.append(ratingMean)\n",
    "validMSE = MSE(preds, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "422ab930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.679945155064233\n"
     ]
    }
   ],
   "source": [
    "answers['Q9'] = validMSE\n",
    "print(validMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "5509bf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q9'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "9826cdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "fcc6ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxBeta = float(max(userBiases.values()))\n",
    "minBeta = float(min(userBiases.values()))\n",
    "\n",
    "maxUser, minUser = '', ''\n",
    "for key in userBiases:\n",
    "    if userBiases[key] == maxBeta:\n",
    "        maxUser= key\n",
    "    elif userBiases[key] == minBeta:\n",
    "        minUser = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "c61b675c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q10'] = [maxUser, minUser, maxBeta, minBeta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "7aca2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert [type(x) for x in answers['Q10']] == [str, str, float, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0a416949",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "ae54cf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "userBiases = defaultdict(float)\n",
    "itemBiases = defaultdict(float)\n",
    "\n",
    "lambs = [10**i for i in range(-8, 8)]\n",
    "mses = []\n",
    "for lamb in lambs:\n",
    "    scipy.optimize.fmin_l_bfgs_b(cost, [alpha] + [0.0]*(nUsers+nItems),\n",
    "                                 derivative, args = (labels, lamb))\n",
    "\n",
    "    preds = []\n",
    "    valid_labels  = []\n",
    "\n",
    "    for u, b, r in ratingsValid:\n",
    "        valid_labels.append(r)\n",
    "        if u in userBiases and b in itemBiases:\n",
    "            preds.append(prediction(u, b))\n",
    "        else:\n",
    "            preds.append(ratingMean)\n",
    "    mses.append(MSE(preds, valid_labels))\n",
    "\n",
    "validMSE = min(mses)\n",
    "lamb = lambs[mses.index(validMSE)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "f1880fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5062059197754023, 1.4019069004683211, 1.4420831895352524, 1.592972247599604, 1.659825399783134, 1.6776278083280678, 1.6799451564369516, 1.6801854306814927]\n"
     ]
    }
   ],
   "source": [
    "answers['Q11'] = (lamb, validMSE)\n",
    "print(mses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "56b09160",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q11'][0])\n",
    "assertFloat(answers['Q11'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "b9bd53b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Rating.csv\", 'w')\n",
    "for l in open(\"pairs_Rating.csv\"):\n",
    "    if l.startswith(\"userID\"): # header\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    userReads = userReadBook[u]\n",
    "    jac = []\n",
    "    for book in userReads:\n",
    "        if b not in BookwasRead:\n",
    "            jac.append(0)\n",
    "        else:\n",
    "            jac.append(Jaccard(BookwasRead[b], BookwasRead[book]))\n",
    "\n",
    "    if max(jac) > threshold_jac and b in return1:\n",
    "        predictions.write(u + '-' + b + \",1\\n\")\n",
    "    else:\n",
    "        predictions.write(u + '-' + b + \",0\\n\")\n",
    "    \n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "93959f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "# Read prediction                                #\n",
    "##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "80f40789",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "abb17ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give a recommendation based on the popularity of the book\n",
    "bookCount = defaultdict(int)\n",
    "totalRead = 0\n",
    "\n",
    "for user,book,_ in readCSV(\"train_Interactions.csv.gz\"):\n",
    "    bookCount[book] += 1\n",
    "    totalRead += 1\n",
    "\n",
    "mostPopular = [(bookCount[x], x) for x in bookCount]\n",
    "mostPopular.sort()\n",
    "mostPopular.reverse()\n",
    "\n",
    "def recommendationOnPopularity(totalRead, mostPopular):\n",
    "    recommendation = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        recommendation.add(i)\n",
    "        if count > totalRead/2: break\n",
    "    return recommendation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "7c9eea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleNegative(allRatings, valid):\n",
    "    validWithNeg = []\n",
    "    userBookList = defaultdict(set)\n",
    "    allBookList = set()\n",
    "    for u, b, r in allRatings:\n",
    "        userBookList[u].add(b)\n",
    "        allBookList.add(b)\n",
    "    \n",
    "    for u, b, r in valid:\n",
    "        validWithNeg.append((u,b,r,1)) # 1 stands for read\n",
    "        aBookUserNeverRead = random.sample(allBookList - userBookList[u], 1)[0]\n",
    "        validWithNeg.append((u, aBookUserNeverRead, 0, 0))\n",
    "    return validWithNeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "d53c2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "validWithNegative = sampleNegative(allRatings, ratingsValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "a40f2af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating accuracy of recommendation\n",
    "recommendation = recommendationOnPopularity(totalRead, mostPopular)\n",
    "correct = 0\n",
    "for u, b, r, read in validWithNegative:\n",
    "    if b in recommendation:\n",
    "        if read: correct +=1\n",
    "    else:\n",
    "        if not read: correct+=1\n",
    "acc1 = correct/len(validWithNegative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "8af7b181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7099\n"
     ]
    }
   ],
   "source": [
    "answers['Q1'] = acc1\n",
    "print(acc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "6839df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "50491907",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "87e03b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommendationWThreshold(totalRead, mostPopular, t):\n",
    "    recommendation = set()\n",
    "    count = 0\n",
    "    for ic, i in mostPopular:\n",
    "        count += ic\n",
    "        recommendation.add(i)\n",
    "        if count > int(totalRead*(t/100)): break\n",
    "    return recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "5ccfffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating accuracy of recommendations with different thresholds\n",
    "thresholds = [i*5 for i in range(1,20)]\n",
    "# Here threshold is defined as the percentage of the popularity book we are recommending (Q1 is 50)\n",
    "acc = []\n",
    "for threshold in thresholds:\n",
    "    recommendation = recommendationWThreshold(totalRead, mostPopular, threshold)\n",
    "    correct = 0\n",
    "    for u, b, r, read in validWithNegative:\n",
    "        if b in recommendation:\n",
    "            if read: correct +=1\n",
    "        else:\n",
    "            if not read: correct+=1\n",
    "    acc.append(correct/len(validWithNegative))\n",
    "    \n",
    "threshold, acc2 = (thresholds[acc.index(max(acc))], max(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "263c16a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 0.7522\n"
     ]
    }
   ],
   "source": [
    "answers['Q2'] = [threshold, acc2]\n",
    "print(threshold, acc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fcb6b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q2'][0])\n",
    "assertFloat(answers['Q2'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b753559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "18cdfd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    result = len(s1 & s2) / len(s1 | s2) if len(s1 | s2) != 0 else 0\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "04a6f2e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating accuracy of recommendations with different jaccard threshold\n",
    "thresholds = [i for i in range(1,51)] # 0.001 - 0.05\n",
    "\n",
    "# Here threshold is defined as the percentage of the jaccard similarity\n",
    "acc = []\n",
    "for threshold in thresholds:\n",
    "    correct = 0\n",
    "    for u, b, r, read in validWithNegative:\n",
    "        jaccard_scores = []\n",
    "        books_u_read = [book_rating[0] for book_rating in ratingsPerUser[u]]\n",
    "        users_read_b = [user_rating[0] for user_rating in ratingsPerItem[b]]\n",
    "        for book in books_u_read:\n",
    "            users_read_book = [user_rating[0] for user_rating in ratingsPerItem[book]]\n",
    "            jaccard_scores.append(Jaccard(set(users_read_b), set(users_read_book)))\n",
    "            \n",
    "        if not jaccard_scores:\n",
    "            jaccard = 0\n",
    "        else:\n",
    "            jaccard = max(max(jaccard_scores), sum(jaccard_scores)/len(jaccard_scores))\n",
    "            \n",
    "        if jaccard >= threshold/1000: # this current book is considered as a good fit\n",
    "            if read: correct +=1\n",
    "        else:\n",
    "            if not read: correct +=1\n",
    "    acc.append(correct/len(validWithNegative))\n",
    "    \n",
    "threshold, acc3 = (thresholds[acc.index(max(acc))], max(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0eb327f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71045\n"
     ]
    }
   ],
   "source": [
    "# Evaluating accuracy of recommendations with the BEST jaccard threshold and popularity threshold\n",
    "\n",
    "bestJaccardThreshold = 3/1000\n",
    "bestPopularityThreshold = 75\n",
    "\n",
    "# Here threshold is defined as the percentage of the jaccard similarity\n",
    "correct = 0\n",
    "for u, b, r, read in validWithNegative:\n",
    "    jaccard_scores = []\n",
    "    books_u_read = [book_rating[0] for book_rating in ratingsPerUser[u]]\n",
    "    users_read_b = [user_rating[0] for user_rating in ratingsPerItem[b]]\n",
    "    for book in books_u_read:\n",
    "        users_read_book = [user_rating[0] for user_rating in ratingsPerItem[book]]\n",
    "        jaccard_scores.append(Jaccard(set(users_read_b), set(users_read_book)))\n",
    "            \n",
    "    if not jaccard_scores:\n",
    "        jaccard = 0\n",
    "    else:\n",
    "        jaccard = max(max(jaccard_scores), sum(jaccard_scores)/len(jaccard_scores))\n",
    "        \n",
    "    if b in recommendation and jaccard >= bestJaccardThreshold:\n",
    "        if read: correct +=1\n",
    "    else:\n",
    "        if not read: correct += 1\n",
    "acc4 = correct/len(validWithNegative)\n",
    "    \n",
    "print(acc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "83ab0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = acc3\n",
    "answers['Q4'] = acc4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "fbdd0c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q3'])\n",
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "3e68cbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = open(\"predictions_Read.csv\", 'w')\n",
    "for l in open(\"pairs_Read.csv\"):\n",
    "    if l.startswith(\"userID\"):\n",
    "        predictions.write(l)\n",
    "        continue\n",
    "    u,b = l.strip().split(',')\n",
    "    # (etc.)\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "297b5c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = \"I confirm that I have uploaded an assignment submission to gradescope\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "b3cb95e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(answers['Q5']) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "839261ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw3.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3000bdde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
