{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fadc0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import numpy\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0e39c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will suppress any warnings, comment out if you'd like to preserve them\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bcdcf1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check formatting of submissions\n",
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42a8d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84568759",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"spoilers.json.gz\", 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4b15a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    dataset.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "043724ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23147241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few utility data structures\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "\n",
    "for d in dataset:\n",
    "    u,i = d['user_id'],d['book_id']\n",
    "    reviewsPerUser[u].append(d)\n",
    "    reviewsPerItem[i].append(d)\n",
    "\n",
    "# Sort reviews per user by timestamp\n",
    "for u in reviewsPerUser:\n",
    "    reviewsPerUser[u].sort(key=lambda x: x['timestamp'])\n",
    "    \n",
    "# Same for reviews per item\n",
    "for i in reviewsPerItem:\n",
    "    reviewsPerItem[i].sort(key=lambda x: x['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "742587d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2012-03-13',\n",
       " '2013-05-06',\n",
       " '2013-09-03',\n",
       " '2015-04-05',\n",
       " '2016-02-10',\n",
       " '2016-05-29']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# E.g. reviews for this user are sorted from earliest to most recent\n",
    "[d['timestamp'] for d in reviewsPerUser['b0d7e561ca59e313b728dc30a5b1862e']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb364612",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "991fbe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y, y_pred):\n",
    "    return numpy.square(numpy.subtract(y,y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "83766457",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "ypred = []\n",
    "for u in reviewsPerUser:\n",
    "    user_data = reviewsPerUser[u]\n",
    "    if len(user_data) > 1:\n",
    "        y.append(user_data[-1]['rating'])\n",
    "        ypred.append(sum([user_data[i]['rating'] for i in range(len(user_data) -1)])/(len(user_data) -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "373cff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1a'] = MSE(y,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bcd5ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c38c9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5131368a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "ypred = []\n",
    "for i in reviewsPerItem:\n",
    "    item_data = reviewsPerItem[i]\n",
    "    if len(item_data) > 1:\n",
    "        y.append(item_data[-1]['rating'])\n",
    "        ypred.append(sum([item_data[ind]['rating'] for ind in range(len(item_data) -1)])/(len(item_data) -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2cccbe4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q1b'] = MSE(y,ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7288fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q1b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0abf5752",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bcd540f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = []\n",
    "for N in [1,2,3]:\n",
    "    y = []\n",
    "    ypred = []\n",
    "    for u in reviewsPerUser:\n",
    "        user_data = reviewsPerUser[u]\n",
    "        if len(user_data) > 1:\n",
    "            y.append(user_data[-1]['rating'])\n",
    "            start = len(user_data) - 1 - N if len(user_data) > (1 + N) else 0\n",
    "            end = len(user_data) - 1\n",
    "            prediction = sum([user_data[i]['rating'] for i in range(start, end)])/(end-start)\n",
    "            ypred.append(prediction)\n",
    "    answers['Q2'].append(MSE(y,ypred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e1b4ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q2'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "206c058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ddd5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature3(N, u): # For a user u and a window size of N\n",
    "    user_data = reviewsPerUser[u]\n",
    "    if len(user_data) > (1 + N):\n",
    "        start = len(user_data) - 1 - N\n",
    "    else:\n",
    "        return []\n",
    "    end = len(user_data) - 1\n",
    "    feature_v = [user_data[i]['rating'] for i in range(start, end)] # theta n - theta 1\n",
    "    feature_v.append(1) # theta 0\n",
    "    feature_v.reverse()\n",
    "    return feature_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8aa5c373",
   "metadata": {},
   "outputs": [],
   "source": [
    " answers['Q3a'] = [feature3(2,dataset[0]['user_id']), feature3(3,dataset[0]['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c57b1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q3a']) == 2\n",
    "assert len(answers['Q3a'][0]) == 3\n",
    "assert len(answers['Q3a'][1]) == 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55691b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4146d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3b'] = []\n",
    "\n",
    "for N in [1,2,3]:\n",
    "    x = []\n",
    "    y = []\n",
    "    ypred = []\n",
    "    for u in reviewsPerUser:\n",
    "        f = feature3(N, u)\n",
    "        if len(f) > 0:\n",
    "            y.append(reviewsPerUser[u][-1]['rating'])\n",
    "            x.append(f)\n",
    "    theta,residuals,rank,s = numpy.linalg.lstsq(x, y, rcond=None)\n",
    "    ypred = numpy.dot(x, theta)\n",
    "    mse = MSE(y,ypred)\n",
    "    answers['Q3b'].append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d512b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q3b'], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ba65fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "4aab34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "globalAverage = [d['rating'] for d in dataset]\n",
    "globalAverage = sum(globalAverage) / len(globalAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2676be3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMeanValue(N, u): # For a user u and a window size of N\n",
    "    user_data = reviewsPerUser[u]\n",
    "    if len(user_data) > 1:\n",
    "        feature_v = [user_data[i]['rating'] for i in range(len(user_data) - 1)] # theta n-x - theta 1\n",
    "        feature_v.reverse()  \n",
    "        avg = sum(feature_v)/len(feature_v)  \n",
    "        if N <= len(feature_v):\n",
    "            return [1] + feature_v[:N]\n",
    "        else:\n",
    "            return [1] + [feature_v[i] if i < len(feature_v) else avg for i in range(N)]\n",
    "    else:\n",
    "        return [1] + [globalAverage for i in range(N)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "270cf89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureMissingValue(N, u):\n",
    "    user_data = reviewsPerUser[u]\n",
    "    if len(user_data) > 1:\n",
    "        feature_v = [[0, user_data[i]['rating']] for i in range(len(user_data) - 1)] # theta n-x - theta 1\n",
    "        feature_v.reverse()  \n",
    "        if N <= len(feature_v):\n",
    "            lst = feature_v[:N]\n",
    "        else:\n",
    "            lst = [feature_v[i] if i < len(feature_v) else [1,0] for i in range(N)]\n",
    "    else:\n",
    "        lst = [[1,0] for i in range(N)]\n",
    "    return [1] + [item for sublist in lst for item in sublist]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58791bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4a'] = [featureMeanValue(10, dataset[0]['user_id']), featureMissingValue(10, dataset[0]['user_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a3c28e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q4a']) == 2\n",
    "assert len(answers['Q4a'][0]) == 11\n",
    "assert len(answers['Q4a'][1]) == 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cbcee03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "73fabbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4b'] = []\n",
    "\n",
    "for featFunc in [featureMeanValue, featureMissingValue]:\n",
    "    N = 10\n",
    "    x = []\n",
    "    y = []\n",
    "    ypred = []\n",
    "    for u in reviewsPerUser:\n",
    "        f = featFunc(N, u)\n",
    "        y.append(reviewsPerUser[u][-1]['rating'])\n",
    "        x.append(f)\n",
    "    theta,residuals,rank,s = numpy.linalg.lstsq(x, y, rcond=None)\n",
    "    ypred = numpy.dot(x, theta)\n",
    "    mse = MSE(y,ypred)\n",
    "    answers['Q4b'].append(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e348489b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers[\"Q4b\"], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c548e8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1cee7eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature5(sentence):\n",
    "    length = len(sentence)\n",
    "    ex_num = sentence.count('!')\n",
    "    cap_num = sum([1 if c.isupper() else 0 for string in sentence for c in string])\n",
    "    return [1, length, ex_num, cap_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "426ca2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "X = []\n",
    "for d in dataset:\n",
    "    for spoiler,sentence in d['review_sentences']:\n",
    "        X.append(feature5(sentence))\n",
    "        y.append(spoiler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a94d7aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5a'] = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c704f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = linear_model.LogisticRegression(C=1.0, class_weight = 'balanced')\n",
    "mod.fit(X, y)\n",
    "ypred = mod.predict(X)\n",
    "TP = sum(numpy.logical_and(ypred, y))\n",
    "FP = sum(numpy.logical_and(ypred, numpy.logical_not(y)))\n",
    "TN = sum(numpy.logical_and(numpy.logical_not(ypred), numpy.logical_not(y)))\n",
    "FN = sum(numpy.logical_and(numpy.logical_not(ypred), y))\n",
    "\n",
    "\n",
    "BER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "116c5b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5b'] = [TP, TN, FP, FN, BER]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c0c96525",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q5a']) == 4\n",
    "assertFloatList(answers['Q5b'], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f826e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "193e94e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def feature6(review):\n",
    "    sentences = d['review_sentences']\n",
    "    features = [sentences[i][0] for i in range(0,5)]\n",
    "    prev_feature = feature5(sentences[5][1])\n",
    "    return features + prev_feature\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a437dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = []\n",
    "X = []\n",
    "for d in dataset:\n",
    "    sentences = d['review_sentences']\n",
    "    if len(sentences) < 6: continue\n",
    "    X.append(feature6(d))\n",
    "    y.append(sentences[5][0])\n",
    "\n",
    "mod = linear_model.LogisticRegression(C=1.0, class_weight = 'balanced')\n",
    "mod.fit(X, y)\n",
    "ypred = mod.predict(X)\n",
    "TP = sum(numpy.logical_and(ypred, y))\n",
    "FP = sum(numpy.logical_and(ypred, numpy.logical_not(y)))\n",
    "TN = sum(numpy.logical_and(numpy.logical_not(ypred), numpy.logical_not(y)))\n",
    "FN = sum(numpy.logical_and(numpy.logical_not(ypred), y))\n",
    "\n",
    "\n",
    "BER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c61a5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6a'] = X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f977c642",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q6b'] = BER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f0be28cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q6a']) == 9\n",
    "assertFloat(answers['Q6b'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3bda0b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8c01c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50/25/25% train/valid/test split\n",
    "Xtrain, Xvalid, Xtest = X[:len(X)//2], X[len(X)//2:(3*len(X))//4], X[(3*len(X))//4:]\n",
    "ytrain, yvalid, ytest = y[:len(X)//2], y[len(X)//2:(3*len(X))//4], y[(3*len(X))//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c253fc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "bers = []\n",
    "for c in [0.01, 0.1, 1, 10, 100]:\n",
    "    mod = linear_model.LogisticRegression(C=c, class_weight = 'balanced')\n",
    "    mod.fit(Xtrain, ytrain)\n",
    "    ypred = mod.predict(Xvalid)\n",
    "    TP = sum(numpy.logical_and(ypred, yvalid))\n",
    "    FP = sum(numpy.logical_and(ypred, numpy.logical_not(yvalid)))\n",
    "    TN = sum(numpy.logical_and(numpy.logical_not(ypred), numpy.logical_not(yvalid)))\n",
    "    FN = sum(numpy.logical_and(numpy.logical_not(ypred), yvalid))\n",
    "    BER = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "    bers.append(BER)\n",
    "    \n",
    "bestC = [0.01, 0.1, 1, 10, 100][bers.index(min(bers))]\n",
    "\n",
    "mod = linear_model.LogisticRegression(C=bestC, class_weight = 'balanced')\n",
    "mod.fit(Xtrain, ytrain)\n",
    "\n",
    "ypred = mod.predict(Xtest)\n",
    "TP = sum(numpy.logical_and(ypred, ytest))\n",
    "FP = sum(numpy.logical_and(ypred, numpy.logical_not(ytest)))\n",
    "TN = sum(numpy.logical_and(numpy.logical_not(ypred), numpy.logical_not(ytest)))\n",
    "FN = sum(numpy.logical_and(numpy.logical_not(ypred), ytest))\n",
    "ber = 1 - 0.5*(TP / (TP + FN) + TN / (TN + FP))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b8389608",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q7'] = bers + [bestC] + [ber]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1d53b5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers['Q7'], 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f06e4f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "38a6c14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    if denom == 0:\n",
    "        return 0\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "57f30ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75/25% train/test split\n",
    "dataTrain = dataset[:15000]\n",
    "dataTest = dataset[15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1d770bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A few utilities\n",
    "\n",
    "itemAverages = defaultdict(list)\n",
    "ratingMean = []\n",
    "\n",
    "for d in dataTrain:\n",
    "    itemAverages[d['book_id']].append(d['rating'])\n",
    "    ratingMean.append(d['rating'])\n",
    "\n",
    "for i in itemAverages:\n",
    "    itemAverages[i] = sum(itemAverages[i]) / len(itemAverages[i])\n",
    "\n",
    "ratingMean = sum(ratingMean) / len(ratingMean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "62952595",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "usersPerItem = defaultdict(set)\n",
    "\n",
    "for d in dataTrain:\n",
    "    u,i = d['user_id'], d['book_id']\n",
    "    reviewsPerUser[u].append(d)\n",
    "    usersPerItem[i].add(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4e0ab533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From my HW2 solution, welcome to reuse\n",
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - itemAverages[i2])\n",
    "        similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        if item in itemAverages:\n",
    "            return itemAverages[item]\n",
    "        else:\n",
    "            return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "db0e1d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "labels = []\n",
    "for d in dataTest:\n",
    "    predictions.append(predictRating(d['user_id'], d['book_id']))\n",
    "    labels.append(d['rating'])\n",
    "    \n",
    "    \n",
    "def MSE(pred_rs, actual_rs):\n",
    "    differences = [(pred_r - actual_r)**2 for pred_r, actual_r in zip(pred_rs,actual_rs)]\n",
    "    return sum(differences) / len(differences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e4891766",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[\"Q8\"] = MSE(predictions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "789b53e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers[\"Q8\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0b298ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5930c59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_0_pred = []\n",
    "item_0_label = []\n",
    "\n",
    "item_1to5_pred = []\n",
    "item_1to5_label = []\n",
    "\n",
    "item_5_pred = []\n",
    "item_5_label = []\n",
    "\n",
    "\n",
    "for d in dataTest:\n",
    "    item_num = len(usersPerItem[d['book_id']])\n",
    "    prediction = predictRating(d['user_id'], d['book_id'])\n",
    "    if item_num == 0:\n",
    "        item_0_pred.append(prediction)\n",
    "        item_0_label.append(d['rating'])\n",
    "    elif item_num <= 5:\n",
    "        item_1to5_pred.append(prediction)\n",
    "        item_1to5_label.append(d['rating'])\n",
    "    else:\n",
    "        item_5_pred.append(prediction)\n",
    "        item_5_label.append(d['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1d2a4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse0 = MSE(item_0_pred, item_0_label)\n",
    "mse1to5 = MSE(item_1to5_pred, item_1to5_label)\n",
    "mse5 = MSE(item_5_pred, item_5_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d269238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[\"Q9\"] = [mse0, mse1to5, mse5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ebfff50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloatList(answers[\"Q9\"], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0dbe10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "e2fea856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6696633366192306\n"
     ]
    }
   ],
   "source": [
    "item_0_pred = []\n",
    "item_0_label = []\n",
    "itemsPerUser = defaultdict(set)\n",
    "user_item_rating = {}\n",
    "for d in dataTrain:\n",
    "    u,i = d['user_id'], d['book_id']\n",
    "    user_item_rating[(u,i)] = d['rating']\n",
    "    itemsPerUser[u].add(i)\n",
    "\n",
    "userItemAverage = {}\n",
    "for u in reviewsPerUser:\n",
    "    if len(reviewsPerUser[u]) ==  0:\n",
    "        userItemAverage[u] = None\n",
    "    else:\n",
    "        lst = []\n",
    "        for d in reviewsPerUser[u]:\n",
    "            lst.append(d['rating'])\n",
    "        \n",
    "        userItemAverage[u] = sum(lst)/len(lst)\n",
    "\n",
    "def predictRatingOptimizing(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - itemAverages[i2])\n",
    "        similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "       # User hasn't rated any similar items\n",
    "        if item in itemAverages:\n",
    "            return itemAverages[item]\n",
    "        else:\n",
    "            # items never seen before\n",
    "            all_other_items = itemsPerUser[user]\n",
    "            if len(all_other_items) == 0:\n",
    "                return ratingMean\n",
    "            else:\n",
    "                all_other_item_rating = [user_item_rating[(user, item)] for item in all_other_items]\n",
    "                return sum(all_other_item_rating) / len(all_other_item_rating)\n",
    "                \n",
    "            return y\n",
    "            \n",
    "for d in dataTest:\n",
    "    item_num = len(usersPerItem[d['book_id']])\n",
    "    prediction = predictRatingOptimizing(d['user_id'], d['book_id'])\n",
    "    if item_num == 0:\n",
    "        item_0_pred.append(prediction)\n",
    "        item_0_label.append(d['rating'])\n",
    "itsMSE = MSE(item_0_pred, item_0_label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "305d3531",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers[\"Q10\"] = (\"While in the previous approach when an item was never seen before, we return the average of all item all user's rating. However, the user who buys this cold star item may have a rating tendency (tend to overall give high ratings or overall give low rating) The approach here is that instead of taking average of all user's rating on all item, if the user is seen before in the training set, we take the average of all this user's previous rating on previous items they bought. If the user also didn't buy other things before, we then still return the old rating average among all user all item\", itsMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d0613500",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(answers[\"Q10\"][0]) == str\n",
    "assertFloat(answers[\"Q10\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "436d2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_midterm.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53acc41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
