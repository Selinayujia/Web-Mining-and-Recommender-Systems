{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import numpy\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.manifold import TSNE\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assertFloat(x):\n",
    "    assert type(float(x)) == float\n",
    "\n",
    "def assertFloatList(items, N):\n",
    "    assert len(items) == N\n",
    "    assert [type(float(x)) for x in items] == [float]*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "f = gzip.open(\"young_adult_20000.json.gz\")\n",
    "for l in f:\n",
    "    d = eval(l)\n",
    "    dataset.append(d)\n",
    "    if len(dataset) >= 20000:\n",
    "        break\n",
    "        \n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = dataset[:10000]\n",
    "d_test = dataset[10000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39761, 330967, 370728)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_wordCount = defaultdict(int)\n",
    "bigram_wordCount = defaultdict(int)\n",
    "uni_bi_wordCount = defaultdict(int)\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "for d in d_train:\n",
    "    r = ''.join([c for c in d['review_text'].lower() \\\n",
    "                 if not c in punctuation])\n",
    "    uni_w = r.split()\n",
    "    bi_w = [' '.join(pair) for pair in list(zip(uni_w[:-1],uni_w[1:]))]\n",
    "    for i in range(len(bi_w)):\n",
    "        unigram_wordCount[uni_w[i]] += 1\n",
    "        bigram_wordCount[bi_w[i]] += 1\n",
    "        uni_bi_wordCount[uni_w[i]] += 1\n",
    "        uni_bi_wordCount[bi_w[i]] += 1\n",
    "        \n",
    "    if(len(uni_w)):\n",
    "        unigram_wordCount[uni_w[-1]] += 1\n",
    "        uni_bi_wordCount[uni_w[-1]] += 1\n",
    "len(unigram_wordCount),len(bigram_wordCount),len(uni_bi_wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_ngrams(dic, cutoff=1000):\n",
    "    counts = [(dic[ngram], ngram) for ngram in dic]\n",
    "    counts.sort()\n",
    "    counts.reverse()\n",
    "    words = [x[1] for x in counts[:cutoff]]\n",
    "    wordId = dict(zip(words, range(len(words))))\n",
    "    return words, wordId\n",
    "\n",
    "ngrams_dic = {'mostCommonUnigrams': get_common_ngrams(unigram_wordCount), \\\n",
    "              'mostCommonBigrams': get_common_ngrams(bigram_wordCount), \\\n",
    "              'mostCommonBoth':get_common_ngrams(uni_bi_wordCount)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(datum, model):\n",
    "    words, wordId = ngrams_dic[model]\n",
    "    \n",
    "    feat = [0]*len(words)\n",
    "    \n",
    "    r = ''.join([c for c in datum['review_text'].lower() if not c in punctuation])\n",
    "    if model == 'mostCommonUnigrams':\n",
    "        r_ngram = r.split()\n",
    "    elif model == 'mostCommonBigrams':\n",
    "        r_ngram = [' '.join(pair) for pair in list(zip(r.split()[:-1],r.split()[1:]))]\n",
    "    else:\n",
    "        r_ngram = r.split() + [' '.join(pair) for pair in list(zip(r.split()[:-1],r.split()[1:]))]\n",
    "    \n",
    "    for w in r_ngram:\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(1) \n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MSE(pred, y):\n",
    "    differences = [(pred - y)**2 for pred, y in zip(pred, y)]\n",
    "    return sum(differences) / len(differences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mostCommonUnigrams, 1.227171642713974,['disappointing', 'boring', 'visuals', 'mangas', 'says'], ['5', 'yourself', 'serie', 'beautifully', 'wait']\n",
      "mostCommonBigrams, 1.2850254712593703,['tuned for', 'thoughts below', 'your next', 'the worst', 'a bad'], ['loved this', '5 stars', 'stay tuned', 'cant wait', 'forget to']\n",
      "mostCommonBoth, 1.220884794454164,['tuned for', 'as promos', 'miss your', 'xoxo katie', 'thoughts below'], ['due to', 'manga happy', 'tuned', 'xoxo', 'reviews as']\n"
     ]
    }
   ],
   "source": [
    "for q, model in ('Q1a', 'mostCommonUnigrams'), ('Q1b', 'mostCommonBigrams'), ('Q1c', 'mostCommonBoth'):\n",
    "    X_train = [feature(d, model) for d in d_train]\n",
    "    y_train = [d['rating'] for d in d_train]\n",
    "    \n",
    "    words, wordId = ngrams_dic[model]\n",
    "\n",
    "    # Regularized regression\n",
    "    clf = linear_model.Ridge(1.0, fit_intercept=False) \n",
    "    clf.fit(X_train, y_train)\n",
    "    theta = clf.coef_\n",
    "    \n",
    "    wordSort = list(zip(theta[:-1], words))\n",
    "    wordSort.sort()\n",
    "    \n",
    "    X_test = [feature(d, model) for d in d_test]\n",
    "    y_test = [d['rating'] for d in d_test]\n",
    "    predictions = clf.predict(X_test)\n",
    "    \n",
    "    mse = MSE(predictions, y_test)\n",
    "    most_negative = [x[1] for x in wordSort[:5]]\n",
    "    most_positive = [x[1] for x in wordSort[-5:]]\n",
    "    \n",
    "    print(f'{model}, {mse},{most_negative}, {most_positive}')\n",
    "    answers[q] = [float(mse), most_negative, most_positive]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q in 'Q1a', 'Q1b', 'Q1c':\n",
    "    assert len(answers[q]) == 3\n",
    "    assertFloat(answers[q][0])\n",
    "    assert [type(x) for x in answers[q][1]] == [str]*5\n",
    "    assert [type(x) for x in answers[q][2]] == [str]*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cosine(x1,x2):\n",
    "    numer = 0\n",
    "    norm1 = 0\n",
    "    norm2 = 0\n",
    "    for a1,a2 in zip(x1,x2):\n",
    "        numer += a1*a2\n",
    "        norm1 += a1**2\n",
    "        norm2 += a2**2\n",
    "    if norm1*norm2:\n",
    "        return numer / math.sqrt(norm1*norm2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, _ = ngrams_dic['mostCommonUnigrams']\n",
    "# df\n",
    "df = defaultdict(int)\n",
    "for d in d_train:\n",
    "    r = ''.join([c for c in d['review_text'].lower() if not c in punctuation])\n",
    "    for w in set(r.split()):\n",
    "        df[w] += 1 # a word only count once in a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf for review 1 in train dataset\n",
    "tf = defaultdict(int)\n",
    "r = ''.join([c for c in d_train[0]['review_text'].lower() if not c in punctuation])\n",
    "for w in r.split():\n",
    "    tf[w] = 1\n",
    "    \n",
    "tfidfQuery = [tf[w] * math.log2(len(dataset) / df[w]) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = []\n",
    "# tf for rest of the review in train dataset\n",
    "for rev in d_train[1:]:\n",
    "    tf = defaultdict(int)\n",
    "    r = ''.join([c for c in rev['review_text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        tf[w] = 1\n",
    "    tfidf2 = [tf[w] * math.log2(len(dataset) / df[w]) for w in words]\n",
    "    similarities.append((Cosine(tfidfQuery, tfidf2), rev['review_text']))\n",
    "    \n",
    "similarities.sort(reverse=True)\n",
    "sim, review = similarities[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q2'] = [sim, review]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q2']) == 2\n",
    "assertFloat(answers['Q2'][0])\n",
    "assert type(answers['Q2'][1]) == str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'dc3763cdb9b2cae805882878eebb6a32',\n",
       " 'book_id': '18471619',\n",
       " 'review_id': '66b2ba840f9bd36d6d27f46136fe4772',\n",
       " 'rating': 3,\n",
       " 'review_text': 'Sherlock Holmes and the Vampires of London \\n Release Date: April 2014 \\n Publisher: Darkhorse Comics \\n Story by: Sylvain Cordurie \\n Art by: Laci \\n Colors by: Axel Gonzabo \\n Cover by: Jean Sebastien Rossbach \\n ISDN: 9781616552664 \\n MSRP: $17.99 Hardcover \\n \"Sherlock Holmes died fighting Professor Moriarty in the Reichenbach Falls. \\n At least, that\\'s what the press claims. \\n However, Holmes is alive and well and taking advantage of his presumed death to travel the globe. \\n Unfortunately, Holmes\\'s plans are thwarted when a plague of vampirism haunts Britain. \\n This book collects Sherlock Holmes and the Vampires of London Volumes 1 and 2, originally created by French publisher Soleil.\" - Darkhorse Comics \\n When I received this copy of \"Sherlock Holmes and the Vampires of London\" I was Ecstatic! The cover art was awesome and it was about two of my favorite things, Sherlock Holmes and Vampires. I couldn\\'t wait to dive into this! \\n Unfortunately, that is where my excitement ended. The story takes place a month after Sherlock Holmes supposed death in his battle with Professor Moriarty. Sherlock\\'s plan to stay hidden and out of site are ruined when on a trip with his brother Mycroft, they stumble on the presence of vampires. That is about as much of Sherlock\\'s character that comes through the book. I can\\'t even tell you the story really because nothing and I mean nothing stuck with me after reading it. I never, ever got the sense of Sherlock Holmes anywhere in this graphic novel, nor any real sense of mystery or crime. It was just Sherlock somehow battling vampires that should have had absolutely no trouble snuffing him out in a fight, but somehow always surviving and holding his own against supernatural, super powerful, blazingly fast creatures. \\n The cover art is awesome and it truly made me excited to read this but everything else feel completely flat for me. I tried telling myself that \"it\\'s a graphic novel, it would be hard to translate mystery, details, emotion\" but then I remembered reading DC Comic\\'s \"Identity Crisis\" and realized that was a load of crap. I know it\\'s unfair to compare the two as \"Identity Crisis\" had popular mystery author Brad Meltzer writing it right? Yeah....no. The standard was set that day and there is more than enough talent out there to create a great story in a graphic novel. \\n That being said, it wasn\\'t a horrible story, it just didn\\'t grip me for feel anything like Sherlock Holmes to me. It was easy enough to follow but I felt no sense of tension, stakes or compassion for any of the characters. \\n As far as the vampires go, it\\'s hard to know what to expect anymore as there are so many different versions these days. This was the more classic version which I personally prefer, but again I didn\\'t find anything that portrayed their dominance, calm confidence or sexuality. There was definitely a presence of their physical prowess but somehow that was lost on me as easily as Sherlock was able to defend himself. I know it, wouldn\\'t do to kill of the main character, but this would have a been a great opportunity to build around the experience and beguiling nature of a vampire that had lived so many years of experience. Another chance to showcase Sherlock\\'s intellect in a battle of wits over strength in something more suitable for this sort of story as apposed to trying to make it feel like an action movie. \\n Maybe I expected to much and hoped to have at least a gripping premise or some sort of interesting plot or mystery but I didn\\'t find it here. This may be a must have for serious Sherlock Holmes fans that have to collect everything about him, but if you are looking for a great story inside a graphic novel, I would have to say pass on this one. \\n That artwork is good, cover is great, story is lacking so I am giving it 2.5 out of 5 stars.',\n",
       " 'date_added': 'Thu Dec 05 10:44:25 -0800 2013',\n",
       " 'date_updated': 'Thu Dec 05 10:45:15 -0800 2013',\n",
       " 'read_at': 'Tue Nov 05 00:00:00 -0800 2013',\n",
       " 'started_at': '',\n",
       " 'n_votes': 0,\n",
       " 'n_comments': 0}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewsPerUser = defaultdict(list)\n",
    "book_to_review = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    reviewDicts.append(d)\n",
    "    book_to_review[d['book_id']].append(d['review_text'])\n",
    "    reviewsPerUser[d['user_id']].append((dateutil.parser.parse(d['date_added']), d['book_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['see full review @ Katie\\'s Corner \\n An amazing slice of life manga. It may lack a bit of romance in comparison with other shoujo mangas, but it compensates in other aspects such as comedy and cuteness. The story is very lively and progresses fast. You will fall in love with Shiharu the babysitter, the twins - Akane and Aoi and well you will love a lot Seiji\\'s on and off modes. If you have to babysit your kid brothers or sisters, which I sometimes do, there are a lot of tricks to keep them occupied and to stop a brawl without shouting. \\n Unfortunately the manga is still ongoing. I say unfortunately, but I\\'m very happy that I do need to say goodbye to my favorite characters. If I am not mistaken there are only 2 more volumes, however, I would like to get more. But, well, we\\'ll see how the author decides. I am really looking forward when Shiharu and Seiji finally reveal their feelings. And they take the twins back to live with them. Wishful thinking? \\n Now that I am sitting down to write my thoughts, I have quite a hard time. In this manga there is nothing spectacular, but just that makes it so amazing. I am not making any sense, am I? The everyday of twin\\'s life, that\\'s what is so amazing and compelling that you do not really want to put down the manga. The little adventures such as a trip to the supermarket, or strawberry hunting or even pancake making are amazing to put away our troubles and have something positive. \\n Even if you do not really like kids, you will Akane and Aoi. Their cuteness is irresistible and trust me you will find yourself liking them more and more though out the manga. However, please bear in mind that Akane and Aoi are just one of a kind. Do not try to find them in all kids out there. Your expectations might be crushed. Mine were, so I am talking about personal experience here. Another thing to say is that the translators did a really amazing job. The kids\\' dialogues are so natural, that you really think that the kids should talk this way. \\n I hope you will take up this amazing manga. It really is spectacular when you come back home stressed and you need something fun and calming to read. I hope you will enjoy it as much as I did and will understand what I meant by \"It is spectacular while having nothing spectacular\" Enjoy it to the fullest! Happy reading and don\\'t forget to share your thoughts with me! \\n XOXO \\n Katie']\n",
      "[\"Review for Volumes 1-5 \\n Wow. What a surprise! \\n Don't't miss THIS ONE if dark erotica in a dystopian world is your genre!\"]\n",
      "[\"Dos webcomics mais fofos que poderiam alguma vez ter sido criados. \\n Com personagens fantasticas caracterizadas por uma arte peculiar e deliciosa, Julia K. transporta-nos para o mundo dos sonhos e pesadelos, onde conhecemos, atraves de Jasper e Ink, o Boogieman (conhecido por nos como o 'bicho-papao') e o Sandman (conhecido por nos como o Joao-Pestana), duas entidades aparentemente tao diferentes mas no fundo tao iguais. \\n Esta e uma historia sobre sonhos, sobre pesadelos, e sobre a importancia que ambos tem no equilibrio deste mundo. \\n Recomendo vivamente a todos os interessados.\"]\n",
      "['I loved this series on the Showa period in Japan. I found the mix of didactic history, popular history, man-on-the-street-view and personal biography to be enjoyable and I feel I learned a lot about Japan. I loved the visuals and felt that the earlier volumes were the best.']\n",
      "[\"More of a three-star story, really, but it gets an extra point for the title. And it also has Wonder Woman and Nightwing in supporting roles and Jason Blood as a major player, and it isn't described terribly well by the title (awesome a title though it is). It also contains Batman and Superman both making some fairly big mistakes, which is what makes it more interesting than a generic fight-some-monsters story.\"]\n"
     ]
    }
   ],
   "source": [
    "#word2vec (gensim)\n",
    "#Tokenize the reviews, so that each review becomes a list of words\n",
    "\n",
    "reviewLists = []\n",
    "for u in reviewsPerUser:\n",
    "    rl = list(reviewsPerUser[u])\n",
    "    rl.sort()\n",
    "    reviewLists.append([x[1] for x in rl])\n",
    "\n",
    "model10 = Word2Vec(reviewLists,\n",
    "                 min_count=1, # Words/items with fewer instances are discarded\n",
    "                 vector_size=10, # Model dimensionality\n",
    "                 window=3, # Window size\n",
    "                 sg=1) # Skip-gram model\n",
    "similarities = []\n",
    "for b in model10.wv.similar_by_word(dataset[0]['book_id'])[:5]:\n",
    "    similarities.append(b)\n",
    "    print(book_to_review[b[0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q3'] = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q3']) == 5\n",
    "assert [type(x[0]) for x in answers['Q3']] == [str]*5\n",
    "assertFloatList([x[1] for x in answers['Q3']], 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([d['rating'] for d in dataset]) / len(dataset)\n",
    "itemAverages = defaultdict(list)\n",
    "reviewsPerUser = defaultdict(list)\n",
    "    \n",
    "for d in dataset:\n",
    "    i = d['book_id']\n",
    "    u = d['user_id']\n",
    "    itemAverages[i].append(d['rating'])\n",
    "    reviewsPerUser[u].append(d)\n",
    "    \n",
    "for i in itemAverages:\n",
    "    itemAverages[i] = sum(itemAverages[i]) / len(itemAverages[i])\n",
    "    \n",
    "def predictRating(user,item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    if not str(item) in model10.wv:\n",
    "        return ratingMean\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'] - itemAverages[i2])\n",
    "        if str(i2) in model10.wv:\n",
    "            similarities.append(Cosine(model10.wv[item], model10.wv[i2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return itemAverages[item] + (sum(weightedRatings) / sum(similarities))\n",
    "    else:\n",
    "        return ratingMean\n",
    "\n",
    "y = []\n",
    "pred_y = []\n",
    "for d in dataset[:1000]:\n",
    "    y.append(d['rating'])\n",
    "    pred_y.append(predictRating(d['user_id'],d['book_id']))\n",
    "mse4 = MSE(pred_y, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q4'] = mse4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "assertFloat(answers['Q4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4080173749719052\n"
     ]
    }
   ],
   "source": [
    "model10 = Word2Vec(reviewLists,\n",
    "                 min_count=5, # Words/items with fewer instances are discarded\n",
    "                 vector_size=10, # Model dimensionality\n",
    "                 window=3, # Window size\n",
    "                 sg=1) # Skip-gram model\n",
    "y = []\n",
    "pred_y = []\n",
    "for d in dataset[:1000]:\n",
    "    y.append(d['rating'])\n",
    "    pred_y.append(predictRating(d['user_id'],d['book_id']))\n",
    "mse5 = MSE(pred_y, y)\n",
    "print(mse5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers['Q5'] = [\"By increase the min_count to five, prevent discarding too many instances that can turn out to be similar\",\n",
    "                 mse5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(answers['Q5']) == 2\n",
    "assert type(answers['Q5'][0]) == str\n",
    "assertFloat(answers['Q5'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"answers_hw4.txt\", 'w')\n",
    "f.write(str(answers) + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
